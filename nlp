{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMNpxIngkbUUlQg4bwa18bE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jungsNN/gitignore/blob/master/nlp\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w5QrBWPAdaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a0542627-3948-431c-87f6-fa6b385b0930"
      },
      "source": [
        "# read in the extracted text file      \n",
        "with open('text8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# print out the first 100 characters\n",
        "print(text[:100])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " anarchism originated as a term of abuse first used against early working class radicals including t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYWa8psqZGze",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU_s2n3iZvd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# read data from text files\n",
        "with open('reviews.txt', 'r') as f:\n",
        "    reviews = f.read()\n",
        "with open('labels.txt', 'r') as f:\n",
        "    labels = f.read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSb5NBEXaPgR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "8884f9b5-9e47-49b0-c380-927f0df11f91"
      },
      "source": [
        "from string import punctuation\n",
        "\n",
        "print(punctuation)\n",
        "\n",
        "# get rid of punctuation\n",
        "reviews = reviews.lower() # lowercase, standardize\n",
        "all_text = ''.join([c for c in reviews if c not in punctuation])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b05c3c6ff91b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# get rid of punctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# lowercase, standardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mall_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreviews\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'reviews' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn-ISt3saUwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split by new lines and spaces\n",
        "reviews_split = all_text.split('\\n')\n",
        "all_text = ' '.join(reviews_split)\n",
        "\n",
        "# create a list of words\n",
        "words = all_text.split()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVfAN9Yxabc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# feel free to use this import \n",
        "from collections import Counter\n",
        "\n",
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(words)\n",
        "\n",
        "## use the dict to tokenize each review in reviews_split\n",
        "vocabs = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_ints = {word: ii for ii, word in enumerate(vocabs, 1)}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xkr7IePaoCw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "d9db6fd4-e324-43f2-b048-fab23240ec0c"
      },
      "source": [
        "## store the tokenized reviews in reviews_ints\n",
        "reviews_ints = []\n",
        "for review in reviews_split:\n",
        "    reviews_ints.append([vocab_ints[word] for word in review.split()])\n",
        "    \n",
        "len(reviews_ints)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk26Db0qapUj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "98576337-d6d1-44cf-e890-42a83f22daa7"
      },
      "source": [
        "# 1=positive, 0=negative label conversion\n",
        "labels_ = labels.split('\\n')\n",
        "len(labels_)\n",
        "\n",
        "encoded_labels = np.array([1 if label=='positive' else 0 for label in labels_])\n",
        "print(encoded_labels[:10])\n",
        "len(encoded_labels)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 1 0 1 0 1 0 1 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgGMZpsfasAX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "683df344-3e36-4503-ad54-644e541ce37c"
      },
      "source": [
        "\n",
        "# outlier review stats - check for extremely short or long lengths \n",
        "review_len = Counter([len(x) for x in reviews_ints])\n",
        "print(\"Zero-length reviews: {}\".format(review_len[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_len)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Zero-length reviews: 1\n",
            "Maximum review length: 2514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcrZgyspatg7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b6a76219-de06-4ef3-8309-63c6a8d58251"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "print(\"Number of reviews before removing outliers: \", len(reviews_ints))\n",
        "non_zero_len = [ii for ii, review in enumerate(reviews_ints) if len(review) > 0]\n",
        "# remove reviews/labels with zero length\n",
        "reviews_ints = [reviews_ints[i] for i in non_zero_len]\n",
        "encoded_labels = np.array([encoded_labels[l] for l in non_zero_len])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(reviews_ints))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of reviews before removing outliers:  25001\n",
            "Number of reviews after removing outliers:  25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76KNCLyiavat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_features(reviews_ints, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's \n",
        "        or truncated to the input seq_length.\n",
        "    '''\n",
        "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "    \n",
        "    for r, review in enumerate(reviews_ints):\n",
        "        features[r, -len(review):] = np.array(review)[:seq_length]\n",
        "\n",
        "    return features"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii7oj6dQaxEr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "31b1a789-e657-4097-f647-f2f1ec4cf59d"
      },
      "source": [
        "# Test your implementation!\n",
        "\n",
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "## test statements - do not change - ##\n",
        "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "# print first 10 values of the first 30 batches \n",
        "print(features[:30,:10])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
            " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   54    10    14   116    60   798   552    71   364     5]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   330   578    34     3   162   748  2731     9   325]\n",
            " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
            " [    1    20     6    76    40     6    58    81    95     5]\n",
            " [   54    10    84   329 26230 46427    63    10    14   614]\n",
            " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   40    26   109 17952  1422     9     1   327     4   125]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [   10   499     1   307 10399    55    74     8    13    30]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkZmrD1JayPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def get_target(words, idx, window_size=5):\n",
        "    ''' Get a list of words in a window around an index. '''\n",
        "    \n",
        "    R = np.random.randint(1, window_size+1)\n",
        "    start = idx - R if (idx - R) > 0 else 0\n",
        "    stop = idx + R\n",
        "    target_words = words[start:idx] + words[idx+1:stop+1]\n",
        "    \n",
        "    return list(target_words)\n",
        "\n",
        "\n",
        "def cos_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
        "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
        "        Here, embedding should be a PyTorch embedding module.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Here we're calculating the cosine similarity between some random words and \n",
        "    # our embedding vectors. With the similarities, we can look at what words are\n",
        "    # close to our random words.\n",
        "    \n",
        "    # sim = (a . b) / |a||b|\n",
        "    \n",
        "    embed_vectors = embedding.weight\n",
        "    \n",
        "    # magnitude of embedding vectors, |b|\n",
        "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
        "    \n",
        "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent \n",
        "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
        "    valid_examples = np.append(valid_examples,\n",
        "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
        "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
        "    \n",
        "    valid_vectors = embedding(valid_examples)\n",
        "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
        "        \n",
        "    return valid_examples, similarities\n",
        "\n",
        "\n",
        "def get_batches(words, batch_size, window_size=5):\n",
        "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
        "    \n",
        "    n_batches = len(words)//batch_size\n",
        "    \n",
        "    # only full batches\n",
        "    words = words[:n_batches*batch_size]\n",
        "    \n",
        "    for idx in range(0, len(words), batch_size):\n",
        "        x, y = [], []\n",
        "        batch = words[idx:idx+batch_size]\n",
        "        for ii in range(len(batch)):\n",
        "            batch_x = batch[ii]\n",
        "            batch_y = get_target(batch, ii, window_size)\n",
        "            y.extend(batch_y)\n",
        "            x.extend([batch_x]*len(batch_y))\n",
        "        yield x, y\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq4WkHyua3gA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NegSkipGram(nn.Module):\n",
        "    def __init__(self, n_vocab, n_embed, noise_dist=None):\n",
        "        super().__init__()\n",
        "       \n",
        "        self.n_vocab = n_vocab\n",
        "        self.n_embed = n_embed\n",
        "        self.noise_dist = noise_dist\n",
        "        \n",
        "        # define embedding layers for input and output words\n",
        "        self.in_embed = nn.Embedding(n_vocab, n_embed)\n",
        "        self.out_embed = nn.Embedding(n_vocab, n_embed)\n",
        "        \n",
        "        # Initialize embedding tables with uniform distribution\n",
        "        # I believe this helps with convergence\n",
        "        self.in_embed.weight.data.uniform_(-1, 1)\n",
        "        self.out_embed.weight.data.uniform_(-1, 1)\n",
        "        \n",
        "    def forward_input(self, input_words):\n",
        "        input_vectors = self.in_embed(input_words)\n",
        "        return input_vectors\n",
        "    \n",
        "    def forward_output(self, output_words):\n",
        "        output_vectors = self.out_embed(output_words)\n",
        "        return output_vectors\n",
        "    \n",
        "    def forward_noise(self, batch_size, n_samples):\n",
        "        \"\"\" Generate noise vectors with shape\n",
        "            (batch_size, n_samples, n_embed)\n",
        "        \"\"\"\n",
        "        if self.noise_dist is None:\n",
        "            # Sample words uniformly\n",
        "            noise_dist = torch.ones(self.n_vocab)\n",
        "        else:\n",
        "            noise_dist = self.noise_dist\n",
        "            \n",
        "        # Sample words from our noise distribution\n",
        "        noise_words = torch.multinomial(noise_dist,\n",
        "                                        batch_size * n_samples,\n",
        "                                        replacement=True)\n",
        "        \n",
        "        device = \"cuda\" if self.out_embed.weight.is_cuda else \"cpu\"\n",
        "        noise_words = noise_words.to(device)\n",
        "        \n",
        "        noise_vectors = self.out_embed(noise_words).view(batch_size, n_samples, self.n_embed)\n",
        "        \n",
        "        return noise_vectors"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A30psfJZa-7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NegSamplingLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input_vectors, output_vectors, noise_vectors):\n",
        "        \n",
        "        batch_size, embed_size = input_vectors.shape\n",
        "        \n",
        "        # Input vectors should be a batch of column vectors\n",
        "        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n",
        "        \n",
        "        # Output vectors should be a batch of row vectors\n",
        "        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n",
        "        \n",
        "        # bmm = batch matrix multiplication\n",
        "        # correct log-sigmoid loss\n",
        "        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n",
        "        out_loss = out_loss.squeeze()\n",
        "        \n",
        "        # incorrect log-sigmoid loss\n",
        "        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n",
        "        noise_loss = noise_loss.squeeze().sum(1)  # sum the losses over the sample of noise vectors\n",
        "\n",
        "        # negate and sum correct and noisy log-sigmoid losses\n",
        "        # return average batch loss\n",
        "        return -(out_loss + noise_loss).mean()\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWn-7uz0bAgZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5e85887f-0f50-4534-87c1-b35a33783b2c"
      },
      "source": [
        "# Get train batches to train the NegSkipGram model\n",
        "import utils\n",
        "\n",
        "# get list of words\n",
        "cleantext = utils.preprocess(text)\n",
        "print(cleantext[:30])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'of', 'the', 'english', 'revolution', 'and', 'the', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjjllDClcYXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "f1784147-8e21-4c76-a162-38ef4bc164a0"
      },
      "source": [
        "# print some stats about this word data\n",
        "print(\"Total words in text: {}\".format(len(cleantext)))\n",
        "print(\"Unique words: {}\".format(len(set(cleantext)))) # `set` removes any duplicate words"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in text: 16680599\n",
            "Unique words: 63641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AUn3gMCccqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "82011435-39c6-45e5-825b-1d1233d13bb0"
      },
      "source": [
        "vocab_to_int, int_to_vocab = utils.create_lookup_tables(cleantext)\n",
        "int_words = [vocab_to_int[word] for word in cleantext]\n",
        "\n",
        "print(int_words[:30])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[447, 3156, 11, 5, 169, 1, 3256, 46, 61, 141, 121, 814, 655, 5369, 142, 0, 1, 0, 137, 734, 2, 0, 1, 0, 162, 734, 4158, 0, 169, 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_IkqhJ6cqCq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6a0afe0d-f2f2-4a2e-fc3b-09bb6fe11263"
      },
      "source": [
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "threshold = 1e-5\n",
        "word_counts = Counter(int_words)\n",
        "#print(list(word_counts.items())[0])  # dictionary of int_words, how many times they appear\n",
        "\n",
        "total_count = len(int_words)\n",
        "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
        "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
        "# discard some frequent words, according to the subsampling equation\n",
        "# create a new list of words for training\n",
        "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
        "\n",
        "print(train_words[:30])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3156, 46, 5369, 734, 162, 734, 4158, 1167, 30, 3157, 16, 278, 235, 2979, 4588, 6553, 4159, 6554, 3158, 3960, 3674, 1272, 8592, 3961, 3674, 3674, 7116, 1272, 1377, 2876]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2M9CQp6c2qD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3b4f015b-05c9-4104-d49b-71c554d7f04a"
      },
      "source": [
        "# test your code!\n",
        "\n",
        "# run this cell multiple times to check for random window selection\n",
        "int_text = [i for i in range(10)]\n",
        "print('Input: ', int_text)\n",
        "idx=5 # word index of interest\n",
        "\n",
        "target = get_target(int_text, idx=idx, window_size=5)\n",
        "print('Target: ', target)  # you should get some indices around the idx"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Target:  [3, 4, 6, 7]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzRV6xy8c6w0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "5d56279d-bb3d-44a5-b9f6-132cfe03a37f"
      },
      "source": [
        "int_text = [i for i in range(20)]\n",
        "x,y = next(get_batches(int_text, batch_size=4, window_size=5))\n",
        "\n",
        "print('x\\n', x)\n",
        "print('y\\n', y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [0, 0, 0, 1, 1, 1, 2, 2, 2, 3]\n",
            "y\n",
            " [1, 2, 3, 0, 2, 3, 0, 1, 3, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kllZJ85oc8tN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def train(model, epochs, train_words, batch_size=512, embed_dim=300,\n",
        "              print_every=1000, lr=0.001):\n",
        "\n",
        "    criterion = NegSamplingLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    step = 0\n",
        "    prevLoss = np.Inf\n",
        "    for e in range(epochs):\n",
        "        for input_words, target_words in get_batches(train_words, batch_size):\n",
        "            step += 1\n",
        "            inputs, targets = torch.LongTensor(input_words), torch.LongTensor(target_words)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "    \n",
        "            # input, outpt, and noise vectors\n",
        "            input_vectors = model.forward_input(inputs)\n",
        "            output_vectors = model.forward_output(targets)\n",
        "            noise_vectors = model.forward_noise(inputs.shape[0], 5)\n",
        "\n",
        "            # negative sampling loss\n",
        "            loss = criterion(input_vectors, output_vectors, noise_vectors)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # loss stats\n",
        "            if step % print_every == 0:\n",
        "                print(\"Epoch: {}/{}\".format(e+1, epochs))\n",
        "                print(\"Loss: \", loss.item()) # avg batch loss\n",
        "\n",
        "                if loss.item() <= prevLoss:\n",
        "                    print('Loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "                    prevLoss,\n",
        "                    loss.item()))\n",
        "    \n",
        "                    checkpoint = {'input_size': model.n_vocab,\n",
        "                                  'output_size': output_vectors.size(0),\n",
        "                                  'n_embed': embed_dim,\n",
        "                                  'state_dict': model.state_dict()}\n",
        "\n",
        "                    torch.save(checkpoint, 'negskipgram.pth')\n",
        "                    prevLoss = loss.item()\n",
        "                valid_examples, valid_similarities = cosine_similarity(model.in_embed, device=device)\n",
        "                _, closest_idxs = valid_similarities.topk(6)\n",
        "\n",
        "                valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
        "                for ii, valid_idx in enumerate(valid_examples):\n",
        "                    closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
        "                    print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
        "                print(\"...\\n\")\n",
        "\n",
        "        losses = loss.item()\n",
        "\n",
        "        if e+1 % 3 == 0:\n",
        "            if losses > prevLoss:\n",
        "                e = epochs\n",
        "\n",
        "                "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twfUn0fMfwMc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "8a3643c7-198f-4408-b86a-c14159c8c113"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "word_freqs = np.array(sorted(freqs.values(), reverse=True))\n",
        "unigram_dist = word_freqs/word_freqs.sum()\n",
        "noise_dist = torch.from_numpy(unigram_dist**(0.75)/np.sum(unigram_dist**(0.75)))\n",
        "\n",
        "embedding_dim = 300\n",
        "nsg_model = NegSkipGram(len(vocab_to_int), embedding_dim, noise_dist=noise_dist).to(device)\n",
        "\n",
        "train(nsg_model, 5, train_words)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 4/5\n",
            "Loss:  10.150032997131348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-43b3aa55f3d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnsg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNegSkipGram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_to_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-c11877fd5af1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, train_words, batch_size, embed_dim, print_every, lr)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# avg batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mprevLoss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     print('Loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n\u001b[1;32m     35\u001b[0m                     \u001b[0mprevLoss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'prevLoss' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQcTndElgxkO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "a4a34370-bc58-419e-c644-5ae760e7848f"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hP1AqOajAPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}